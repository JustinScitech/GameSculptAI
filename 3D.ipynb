{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e820e3e2ad3b4df6ad1e690e1fb1ed32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m guidance_scale \u001b[39m=\u001b[39m \u001b[39m15.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdog\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m latents \u001b[39m=\u001b[39m sample_latents(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     diffusion\u001b[39m=\u001b[39;49mdiffusion,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     guidance_scale\u001b[39m=\u001b[39;49mguidance_scale,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     model_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(texts\u001b[39m=\u001b[39;49m[prompt] \u001b[39m*\u001b[39;49m batch_size),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     clip_denoised\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     use_fp16\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     use_karras\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     karras_steps\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     sigma_min\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     sigma_max\u001b[39m=\u001b[39;49m\u001b[39m160\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     s_churn\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m render_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnerf\u001b[39m\u001b[39m'\u001b[39m  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m  \u001b[39m# this is the size of the renders; higher values take longer to render.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/diffusion/sample.py:62\u001b[0m, in \u001b[0;36msample_latents\u001b[0;34m(batch_size, model, diffusion, model_kwargs, guidance_scale, clip_denoised, use_fp16, use_karras, karras_steps, sigma_min, sigma_max, s_churn, device, progress)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39mdevice\u001b[39m.\u001b[39mtype, enabled\u001b[39m=\u001b[39muse_fp16):\n\u001b[1;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m use_karras:\n\u001b[0;32m---> 62\u001b[0m         samples \u001b[39m=\u001b[39m karras_sample(\n\u001b[1;32m     63\u001b[0m             diffusion\u001b[39m=\u001b[39;49mdiffusion,\n\u001b[1;32m     64\u001b[0m             model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     65\u001b[0m             shape\u001b[39m=\u001b[39;49msample_shape,\n\u001b[1;32m     66\u001b[0m             steps\u001b[39m=\u001b[39;49mkarras_steps,\n\u001b[1;32m     67\u001b[0m             clip_denoised\u001b[39m=\u001b[39;49mclip_denoised,\n\u001b[1;32m     68\u001b[0m             model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs,\n\u001b[1;32m     69\u001b[0m             device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     70\u001b[0m             sigma_min\u001b[39m=\u001b[39;49msigma_min,\n\u001b[1;32m     71\u001b[0m             sigma_max\u001b[39m=\u001b[39;49msigma_max,\n\u001b[1;32m     72\u001b[0m             s_churn\u001b[39m=\u001b[39;49ms_churn,\n\u001b[1;32m     73\u001b[0m             guidance_scale\u001b[39m=\u001b[39;49mguidance_scale,\n\u001b[1;32m     74\u001b[0m             progress\u001b[39m=\u001b[39;49mprogress,\n\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         internal_batch_size \u001b[39m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/diffusion/k_diffusion.py:113\u001b[0m, in \u001b[0;36mkarras_sample\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mkarras_sample\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    112\u001b[0m     last \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m karras_sample_progressive(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[1;32m    114\u001b[0m         last \u001b[39m=\u001b[39;49m x[\u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m last\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/diffusion/k_diffusion.py:181\u001b[0m, in \u001b[0;36mkarras_sample_progressive\u001b[0;34m(diffusion, model, shape, steps, clip_denoised, progress, model_kwargs, device, sigma_min, sigma_max, rho, sampler, s_churn, s_tmin, s_tmax, s_noise, guidance_scale)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     guided_denoiser \u001b[39m=\u001b[39m denoiser\n\u001b[0;32m--> 181\u001b[0m \u001b[39mfor\u001b[39;49;00m obj \u001b[39min\u001b[39;49;00m sample_fn(\n\u001b[1;32m    182\u001b[0m     guided_denoiser,\n\u001b[1;32m    183\u001b[0m     x_T,\n\u001b[1;32m    184\u001b[0m     sigmas,\n\u001b[1;32m    185\u001b[0m     progress\u001b[39m=\u001b[39;49mprogress,\n\u001b[1;32m    186\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msampler_args,\n\u001b[1;32m    187\u001b[0m ):\n\u001b[1;32m    188\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(diffusion, GaussianDiffusion):\n\u001b[1;32m    189\u001b[0m         \u001b[39myield\u001b[39;49;00m diffusion\u001b[39m.\u001b[39;49munscale_out_dict(obj)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/diffusion/k_diffusion.py:265\u001b[0m, in \u001b[0;36msample_heun\u001b[0;34m(denoiser, x, sigmas, progress, s_churn, s_tmin, s_tmax, s_noise)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m gamma \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    264\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m eps \u001b[39m*\u001b[39m (sigma_hat\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m sigmas[i] \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m--> 265\u001b[0m denoised \u001b[39m=\u001b[39m denoiser(x, sigma_hat \u001b[39m*\u001b[39;49m s_in)\n\u001b[1;32m    266\u001b[0m d \u001b[39m=\u001b[39m to_d(x, sigma_hat, denoised)\n\u001b[1;32m    267\u001b[0m \u001b[39myield\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m: x, \u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m: i, \u001b[39m\"\u001b[39m\u001b[39msigma\u001b[39m\u001b[39m\"\u001b[39m: sigmas[i], \u001b[39m\"\u001b[39m\u001b[39msigma_hat\u001b[39m\u001b[39m\"\u001b[39m: sigma_hat, \u001b[39m\"\u001b[39m\u001b[39mpred_xstart\u001b[39m\u001b[39m\"\u001b[39m: denoised}\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/diffusion/k_diffusion.py:173\u001b[0m, in \u001b[0;36mkarras_sample_progressive.<locals>.guided_denoiser\u001b[0;34m(x_t, sigma)\u001b[0m\n\u001b[1;32m    171\u001b[0m x_t \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([x_t, x_t], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    172\u001b[0m sigma \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([sigma, sigma], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 173\u001b[0m x_0 \u001b[39m=\u001b[39m denoiser(x_t, sigma)\n\u001b[1;32m    174\u001b[0m cond_x_0, uncond_x_0 \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39msplit(x_0, \u001b[39mlen\u001b[39m(x_0) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    175\u001b[0m x_0 \u001b[39m=\u001b[39m uncond_x_0 \u001b[39m+\u001b[39m guidance_scale \u001b[39m*\u001b[39m (cond_x_0 \u001b[39m-\u001b[39m uncond_x_0)\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/diffusion/k_diffusion.py:160\u001b[0m, in \u001b[0;36mkarras_sample_progressive.<locals>.denoiser\u001b[0;34m(x_t, sigma)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdenoiser\u001b[39m(x_t, sigma):\n\u001b[0;32m--> 160\u001b[0m     _, denoised \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdenoise(\n\u001b[1;32m    161\u001b[0m         x_t, sigma, clip_denoised\u001b[39m=\u001b[39;49mclip_denoised, model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m denoised\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/diffusion/k_diffusion.py:105\u001b[0m, in \u001b[0;36mGaussianToKarrasDenoiser.denoise\u001b[0;34m(self, x_t, sigmas, clip_denoised, model_kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m t \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m    100\u001b[0m     [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma_to_t(sigma) \u001b[39mfor\u001b[39;00m sigma \u001b[39min\u001b[39;00m sigmas\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()],\n\u001b[1;32m    101\u001b[0m     dtype\u001b[39m=\u001b[39mth\u001b[39m.\u001b[39mlong,\n\u001b[1;32m    102\u001b[0m     device\u001b[39m=\u001b[39msigmas\u001b[39m.\u001b[39mdevice,\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    104\u001b[0m c_in \u001b[39m=\u001b[39m append_dims(\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m (sigmas\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m, x_t\u001b[39m.\u001b[39mndim)\n\u001b[0;32m--> 105\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdiffusion\u001b[39m.\u001b[39;49mp_mean_variance(\n\u001b[1;32m    106\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, x_t \u001b[39m*\u001b[39;49m c_in, t, clip_denoised\u001b[39m=\u001b[39;49mclip_denoised, model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, out[\u001b[39m\"\u001b[39m\u001b[39mpred_xstart\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/diffusion/gaussian_diffusion.py:333\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m B, C \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    332\u001b[0m \u001b[39massert\u001b[39;00m t\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (B,)\n\u001b[0;32m--> 333\u001b[0m model_output \u001b[39m=\u001b[39m model(x, t, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    334\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model_output, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    335\u001b[0m     model_output, extra \u001b[39m=\u001b[39m model_output\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/models/generation/latent_diffusion.py:21\u001b[0m, in \u001b[0;36mSplitVectorDiffusion.forward\u001b[0;34m(self, x, t, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m h \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_ctx, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m pre_channels \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrapped(h, t, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     22\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     h\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m pre_channels \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     24\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mexpected twice as many outputs for variance prediction\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m eps, var \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mchunk(h, \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/models/generation/transformer.py:298\u001b[0m, in \u001b[0;36mCLIPImagePointDiffusionTransformer.forward\u001b[0;34m(self, x, t, images, texts, embeddings)\u001b[0m\n\u001b[1;32m    295\u001b[0m clip_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_embed(clip_out)\n\u001b[1;32m    297\u001b[0m cond \u001b[39m=\u001b[39m [(clip_embed, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_cond), (t_embed, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_token_cond)]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_with_cond(x, cond)\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/models/generation/transformer.py:231\u001b[0m, in \u001b[0;36mPointDiffusionTransformer._forward_with_cond\u001b[0;34m(self, x, cond_as_token)\u001b[0m\n\u001b[1;32m    228\u001b[0m     h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(extra_tokens \u001b[39m+\u001b[39m [h], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    230\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(h)\n\u001b[0;32m--> 231\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(h)\n\u001b[1;32m    232\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_post(h)\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(extra_tokens):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/models/generation/transformer.py:147\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    146\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresblocks:\n\u001b[0;32m--> 147\u001b[0m         x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m    148\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/models/generation/transformer.py:109\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 109\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(x))\n\u001b[1;32m    110\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x))\n\u001b[1;32m    111\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/models/generation/transformer.py:42\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     41\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_qkv(x)\n\u001b[0;32m---> 42\u001b[0m     x \u001b[39m=\u001b[39m checkpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention, (x,), (), \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     43\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(x)\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/models/nn/checkpoint.py:24\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m flag:\n\u001b[1;32m     23\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(inputs) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(params)\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(func, \u001b[39mlen\u001b[39;49m(inputs), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39minputs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/cuda/amp/autocast_mode.py:113\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m cast_inputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fwd_used_autocast \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_enabled()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m fwd(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     autocast_context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_enabled()\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/models/nn/checkpoint.py:39\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, length, *args)\u001b[0m\n\u001b[1;32m     37\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39minput_tensors, \u001b[39m*\u001b[39minput_params)\n\u001b[1;32m     38\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 39\u001b[0m     output_tensors \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39;49mrun_function(\u001b[39m*\u001b[39;49minput_tensors)\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m output_tensors\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/models/generation/transformer.py:75\u001b[0m, in \u001b[0;36mQKVMultiheadAttention.forward\u001b[0;34m(self, qkv)\u001b[0m\n\u001b[1;32m     73\u001b[0m qkv \u001b[39m=\u001b[39m qkv\u001b[39m.\u001b[39mview(bs, n_ctx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m q, k, v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msplit(qkv, attn_ch, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49meinsum(\n\u001b[1;32m     76\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mbthc,bshc->bhts\u001b[39;49m\u001b[39m\"\u001b[39;49m, q \u001b[39m*\u001b[39;49m scale, k \u001b[39m*\u001b[39;49m scale\n\u001b[1;32m     77\u001b[0m )  \u001b[39m# More stable with f16 than dividing afterwards\u001b[39;00m\n\u001b[1;32m     78\u001b[0m wdtype \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mdtype\n\u001b[1;32m     79\u001b[0m weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(weight\u001b[39m.\u001b[39mfloat(), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtype(wdtype)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    379\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget, decode_latent_mesh\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('text300M', device=device)\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))\n",
    "batch_size = 1\n",
    "guidance_scale = 15.0\n",
    "prompt = \"dog\"\n",
    "latents = sample_latents(\n",
    "    batch_size=batch_size,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    guidance_scale=guidance_scale,\n",
    "    model_kwargs=dict(texts=[prompt] * batch_size),\n",
    "    progress=True,\n",
    "    clip_denoised=True,\n",
    "    use_fp16=True,\n",
    "    use_karras=True,\n",
    "    karras_steps=64,\n",
    "    sigma_min=1e-3,\n",
    "    sigma_max=160,\n",
    "    s_churn=0,\n",
    ")\n",
    "render_mode = 'nerf'  \n",
    "size = 64  # this is the size of the renders; higher values take longer to render.\n",
    "cameras = create_pan_cameras(size, device)\n",
    "for i, latent in enumerate(latents):\n",
    "    with open(f'mesh_{i}.ply', 'wb') as f:\n",
    "        decode_latent_mesh(xm, latent).tri_mesh().write_ply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "User specified an unsupported autocast device_type 'mps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m guidance_scale \u001b[39m=\u001b[39m \u001b[39m15.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdog\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m latents \u001b[39m=\u001b[39m sample_latents(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     diffusion\u001b[39m=\u001b[39;49mdiffusion,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     guidance_scale\u001b[39m=\u001b[39;49mguidance_scale,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     model_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(texts\u001b[39m=\u001b[39;49m[prompt] \u001b[39m*\u001b[39;49m batch_size),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     clip_denoised\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     use_fp16\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# Note: Consider compatibility of fp16 with MPS\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     use_karras\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     karras_steps\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     sigma_min\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     sigma_max\u001b[39m=\u001b[39;49m\u001b[39m160\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     s_churn\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m render_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnerf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenny/Documents/Machine_Learning/Gen3D/shap-e/3D.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Gen3D/shap-e/shap_e/diffusion/sample.py:60\u001b[0m, in \u001b[0;36msample_latents\u001b[0;34m(batch_size, model, diffusion, model_kwargs, guidance_scale, clip_denoised, use_fp16, use_karras, karras_steps, sigma_min, sigma_max, s_churn, device, progress)\u001b[0m\n\u001b[1;32m     57\u001b[0m         model_kwargs[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([v, torch\u001b[39m.\u001b[39mzeros_like(v)], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     59\u001b[0m sample_shape \u001b[39m=\u001b[39m (batch_size, model\u001b[39m.\u001b[39md_latent)\n\u001b[0;32m---> 60\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mautocast(device_type\u001b[39m=\u001b[39;49mdevice\u001b[39m.\u001b[39;49mtype, enabled\u001b[39m=\u001b[39;49muse_fp16):\n\u001b[1;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m use_karras:\n\u001b[1;32m     62\u001b[0m         samples \u001b[39m=\u001b[39m karras_sample(\n\u001b[1;32m     63\u001b[0m             diffusion\u001b[39m=\u001b[39mdiffusion,\n\u001b[1;32m     64\u001b[0m             model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m             progress\u001b[39m=\u001b[39mprogress,\n\u001b[1;32m     75\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/amp/autocast_mode.py:241\u001b[0m, in \u001b[0;36mautocast.__init__\u001b[0;34m(self, device_type, dtype, enabled, cache_enabled)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfast_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcustom_device_mod\u001b[39m.\u001b[39mget_autocast_dtype()\n\u001b[1;32m    240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUser specified an unsupported autocast device_type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m     )\n\u001b[1;32m    244\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    246\u001b[0m     enabled\n\u001b[1;32m    247\u001b[0m     \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mcommon\u001b[39m.\u001b[39mamp_definitely_not_available()\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: User specified an unsupported autocast device_type 'mps'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget\n",
    "\n",
    "# Check if MPS is available and use it; otherwise, fall back to CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('text300M', device=device)\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))\n",
    "batch_size = 1\n",
    "guidance_scale = 15.0\n",
    "prompt = \"dog\"\n",
    "latents = sample_latents(\n",
    "    batch_size=batch_size,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    guidance_scale=guidance_scale,\n",
    "    model_kwargs=dict(texts=[prompt] * batch_size),\n",
    "    progress=True,\n",
    "    clip_denoised=True,\n",
    "    use_fp16=True,  # Note: Consider compatibility of fp16 with MPS\n",
    "    use_karras=True,\n",
    "    karras_steps=64,\n",
    "    sigma_min=1e-3,\n",
    "    sigma_max=160,\n",
    "    s_churn=0,\n",
    ")\n",
    "render_mode = 'nerf'\n",
    "size = 16\n",
    "cameras = create_pan_cameras(size, device)\n",
    "for i, latent in enumerate(latents):\n",
    "    images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    display(gif_widget(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-14.0-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.platform())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
